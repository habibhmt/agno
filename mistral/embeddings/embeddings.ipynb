{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92587c8a-897d-4fc7-a62a-2988da97e45d",
      "metadata": {
        "id": "92587c8a-897d-4fc7-a62a-2988da97e45d"
      },
      "source": [
        "# Embeddings\n",
        "\n",
        "Embeddings are vectorial representations of text that capture the semantic meaning of paragraphs through their position in a high dimensional vector space. Mistral Embeddings API offers cutting-edge, state-of-the-art embeddings for text, which can be used for many NLP tasks. In this guide, we will cover the fundamentals of the Mistral embeddings API, including how to measure the distance between text embeddings, and explore its main use cases:  clustering and classification.\n",
        "\n",
        "## Mistral Embeddings API\n",
        "To generate text embeddings using Mistral’s embeddings API, we can make a request to the API endpoint and specify the embedding model `mistral-embed`, along with providing a list of input text. The API will then return the corresponding embeddings as numerical vectors, which can be used for further analysis or processing in NLP applications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4312b3ac-a70b-443c-98f9-f7f5d7c1bd33",
      "metadata": {
        "id": "4312b3ac-a70b-443c-98f9-f7f5d7c1bd33",
        "outputId": "e41758ce-eee4-4abd-8b4d-f5811bc3aa5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mistralai\n",
            "  Downloading mistralai-1.5.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting eval-type-backport>=0.2.0 (from mistralai)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.28.1)\n",
            "Collecting jsonpath-python>=1.0.6 (from mistralai)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pydantic>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.10.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.8.2)\n",
            "Collecting typing-inspect>=0.9.0 (from mistralai)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->mistralai) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.0->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.0->mistralai) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.0->mistralai) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.9.0->mistralai)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->mistralai) (1.3.1)\n",
            "Downloading mistralai-1.5.1-py3-none-any.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, jsonpath-python, eval-type-backport, typing-inspect, mistralai\n",
            "Successfully installed eval-type-backport-0.2.2 jsonpath-python-1.0.6 mistralai-1.5.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mistralai seaborn numpy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7c28ce18-065d-4379-8bbb-214479238c77",
      "metadata": {
        "id": "7c28ce18-065d-4379-8bbb-214479238c77",
        "outputId": "7b374416-5066-4935-d679-e6efa52d7b85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'MISTRAL_API_KEY'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f352f1e855a1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmistralai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMistral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MISTRAL_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mistral-embed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'MISTRAL_API_KEY'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from mistralai import Mistral\n",
        "\n",
        "api_key = os.environ[\"30A7213C-541B-4886-B1C8-1DAEE10870AC\"]\n",
        "model = \"mistral-embed\"\n",
        "\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "embeddings_batch_response = client.embeddings.create(\n",
        "    model=model,\n",
        "    inputs=[\"Embed this sentence.\", \"As well as this one.\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f410f39-5b57-4284-af91-7be21abf79c4",
      "metadata": {
        "id": "0f410f39-5b57-4284-af91-7be21abf79c4"
      },
      "source": [
        "\n",
        "The output is a EmbeddingResponse object with the embeddings and the token usage information.\n",
        "\n",
        "```\n",
        "Data(id='eb4c2c739780415bb3af4e47580318cc', object='list', data=[EmbeddingObject(object='embedding', embedding=[-0.0165863037109375,...], index=0), Data(object='embedding', embedding=[-0.0234222412109375,...], index=1)], model='mistral-embed', usage=EmbeddingResponseUsage(prompt_tokens=15, total_tokens=15, completion_tokens=0))\n",
        "\n",
        "```\n",
        "Let’s take a look of the length of the first embedding:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "709bfa0d-664c-4902-8de5-f96a22232ca7",
      "metadata": {
        "id": "709bfa0d-664c-4902-8de5-f96a22232ca7"
      },
      "outputs": [],
      "source": [
        "len(embeddings_batch_response.data[0].embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09d4267d-8ab1-4c7c-91c0-0f1d567e295a",
      "metadata": {
        "id": "09d4267d-8ab1-4c7c-91c0-0f1d567e295a"
      },
      "source": [
        "It returns 1024, which means that our embedding dimension is 1024. The `mistral-embed` model generates embedding vectors of dimension 1024 for each text string, regardless of the text length.  It’s worth noting that while higher dimensional embeddings can better capture text information and improve the performance of NLP tasks, they may require more computational resources for hosting and inference, and may result in increased latency and memory usage for storing and processing these embeddings. This trade-off between performance and computational resources should be considered when designing NLP systems that rely on text embeddings.\n",
        "\n",
        "### Distance measures\n",
        "\n",
        "In the realm of text embeddings, texts with similar meanings or context tend to be located in closer proximity to each other within this space, as measured by the distance between their vectors. This is due to the fact that the model has learned to group semantically related texts together during the training process.\n",
        "\n",
        "Let’s take a look at a simple example. To simplify working with text embeddings, we can wrap the embedding API in this function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95c2bdf-f08a-4d36-825a-0f78ce82ec7d",
      "metadata": {
        "id": "c95c2bdf-f08a-4d36-825a-0f78ce82ec7d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "def get_text_embedding(inputs):\n",
        "    embeddings_batch_response = client.embeddings.create(\n",
        "        model=model,\n",
        "        inputs=inputs\n",
        "    )\n",
        "    return embeddings_batch_response.data[0].embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7206c037-97d3-4eee-a510-46f6bff1d78b",
      "metadata": {
        "id": "7206c037-97d3-4eee-a510-46f6bff1d78b"
      },
      "source": [
        "Suppose we have two sentences: one about cats and the other about books.  We want to find how similar each sentence is to the reference sentence \"Books are mirrors: You only see in them what you already have inside you\". We can see that the distance between the reference sentence embeddings and the book sentence embeddings is smaller than the distance between the reference sentence embeddings and the cat sentence embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd16565e-272b-46b2-884d-3c61e91cbe52",
      "metadata": {
        "id": "fd16565e-272b-46b2-884d-3c61e91cbe52"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"A home without a cat — and a well-fed, well-petted and properly revered cat — may be a perfect home, perhaps, but how can it prove title?\",\n",
        "    \"I think books are like people, in the sense that they’ll turn up in your life when you most need them\"\n",
        "]\n",
        "embeddings = [get_text_embedding([t]) for t in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1587f75-3f36-46aa-86d8-7c2fe400be5e",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "b1587f75-3f36-46aa-86d8-7c2fe400be5e"
      },
      "outputs": [],
      "source": [
        "reference_sentence = \"Books are mirrors: You only see in them what you already have inside you\"\n",
        "reference_embedding = get_text_embedding([reference_sentence])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d7b2d20-a920-4171-95de-3f0812803321",
      "metadata": {
        "id": "8d7b2d20-a920-4171-95de-3f0812803321"
      },
      "outputs": [],
      "source": [
        "for t, e in zip(sentences, embeddings):\n",
        "    distance = euclidean_distances([e], [reference_embedding])\n",
        "    print(t, distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdfb86a9-7664-464a-9cc2-854993b72238",
      "metadata": {
        "id": "bdfb86a9-7664-464a-9cc2-854993b72238"
      },
      "source": [
        "In our example above, we used the Euclidean distance to measure the distance between embedding vectors (note that since Mistral embeddings are norm 1, cosine similarity, dot product or Euclidean distance are all equivalent).\n",
        "\n",
        "### Paraphrase detection\n",
        "\n",
        "Another potential use case is paraphrase detection. In this simple example, we have a list of three sentences, and we would like to find out if any of the two sentences are paraphrases of each other. If the distance between two sentence embeddings is small, it suggests that the two sentences are semantically similar and could be potential paraphrases.\n",
        "\n",
        "Result suggests that the first two sentences are semantically similar and could be potential paraphrases, whereas the third sentence is more different. This is just a super simple example. But this approach can be extended to more complex situations in real-world applications, such as detecting paraphrases in social media posts, news articles, or customer reviews.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d610ebd-e0a0-4435-ab1d-07216b214b89",
      "metadata": {
        "id": "2d610ebd-e0a0-4435-ab1d-07216b214b89"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    'Have a safe happy Memorial Day weekend everyone',\n",
        "    'To all our friends at Whatsit Productions Films enjoy a safe happy Memorial Day weekend',\n",
        "    'Where can I find the best cheese?'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5938ef92-a83d-4ca1-a32c-5ecd32ba9ae8",
      "metadata": {
        "id": "5938ef92-a83d-4ca1-a32c-5ecd32ba9ae8"
      },
      "outputs": [],
      "source": [
        "sentence_embeddings = [get_text_embedding([t]) for t in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f5449e4-ee20-4bcc-aa79-a44f084f6b97",
      "metadata": {
        "id": "6f5449e4-ee20-4bcc-aa79-a44f084f6b97"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "sentence_embeddings_pairs = list(itertools.combinations(sentence_embeddings, 2))\n",
        "sentence_pairs = list(itertools.combinations(sentences, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae95833b-3013-4275-af57-71604627482b",
      "metadata": {
        "id": "ae95833b-3013-4275-af57-71604627482b"
      },
      "outputs": [],
      "source": [
        "for s, e in zip(sentence_pairs, sentence_embeddings_pairs):\n",
        "    print(s, euclidean_distances([e[0]], [e[1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "117ab6ab-1ec7-4030-a168-13d753a6a3d2",
      "metadata": {
        "id": "117ab6ab-1ec7-4030-a168-13d753a6a3d2"
      },
      "source": [
        "### Batch processing\n",
        "\n",
        "The Mistral Embeddings API is designed to process text in batches for improved efficiency and speed. In this example, we will demonstrate this by loading the Symptom2Disease dataset from [Kaggle](https://www.kaggle.com/datasets/niyarrbarman/symptom2disease), which contains 1200 rows with two columns: \"label\" and \"text\". The \"label\" column indicates the disease category, while the \"text\" column describes the symptoms associated with that disease.\n",
        "\n",
        "We wrote a function `get_embeddings_by_chunks` that splits data into chunks and then sends each chunk to the Mistral embedding API to get the embeddings. Then we saved the embeddings as a new column in the dataframe. Note that the Mistral Embeddings API will provide auto-chunking in the future, so that users don’t need to manually split the data into chunks before sending it to the API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fa58906-41b1-4b50-96ca-8457175de37a",
      "metadata": {
        "id": "3fa58906-41b1-4b50-96ca-8457175de37a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/mistralai/cookbook/main/data/Symptom2Disease.csv\", index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0c7821a-da97-4baa-afa5-aa023a5298b4",
      "metadata": {
        "id": "e0c7821a-da97-4baa-afa5-aa023a5298b4"
      },
      "outputs": [],
      "source": [
        "def get_embeddings_by_chunks(data, chunk_size):\n",
        "    chunks = [data[x : x + chunk_size] for x in range(0, len(data), chunk_size)]\n",
        "    embeddings_response = [\n",
        "        client.embeddings.create(model=model, inputs=c) for c in chunks\n",
        "    ]\n",
        "    return [d.embedding for e in embeddings_response for d in e.data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0282432-3656-4eb5-9e6a-2d1358187b33",
      "metadata": {
        "id": "b0282432-3656-4eb5-9e6a-2d1358187b33"
      },
      "outputs": [],
      "source": [
        "df['embeddings'] = get_embeddings_by_chunks(df['text'].tolist(), 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d714a9e-783c-4385-95ee-5f2f3fbeb7fe",
      "metadata": {
        "id": "7d714a9e-783c-4385-95ee-5f2f3fbeb7fe"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77ac1d38-ddf1-40c6-855f-1065405f0ec3",
      "metadata": {
        "id": "77ac1d38-ddf1-40c6-855f-1065405f0ec3"
      },
      "source": [
        "## t-SNE embeddings visualization\n",
        "\n",
        "We mentioned previously that our embeddings have 1024 dimensions, which makes them impossible to visualize directly. Thus, in order to visualize our embeddings, we can use a dimensionality reduction technique such as t-SNE to project our embeddings into a lower-dimensional space that is easier to visualize.\n",
        "\n",
        "In this example, we transform our embeddings to 2 dimensions and create a 2D scatter plot showing the relationships among embeddings of different diseases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2badca7-ec02-421e-99a8-2bc44fdd42af",
      "metadata": {
        "id": "e2badca7-ec02-421e-99a8-2bc44fdd42af"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=0).fit_transform(np.array(df['embeddings'].to_list()))\n",
        "ax = sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=np.array(df['label'].to_list()))\n",
        "sns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8211936e-f5cf-4eca-91fb-05c65418f20d",
      "metadata": {
        "id": "8211936e-f5cf-4eca-91fb-05c65418f20d"
      },
      "source": [
        "#### Comparison with fasttext\n",
        "\n",
        "We can compare it with fastText, a popular open-source embeddings model. However, when examining the t-SNE embeddings plot, we notice that fastText embeddings fail to create clear separations between data points with matching labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00df9498-8c5f-4322-91bf-4384bc70902b",
      "metadata": {
        "id": "00df9498-8c5f-4322-91bf-4384bc70902b"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ca9418-d355-4c3c-9a1d-03753262cb1d",
      "metadata": {
        "id": "e9ca9418-d355-4c3c-9a1d-03753262cb1d"
      },
      "outputs": [],
      "source": [
        "import fasttext.util\n",
        "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "ft = fasttext.load_model('cc.en.300.bin')\n",
        "\n",
        "df['fasttext_embeddings'] = df['text'].apply(lambda x: ft.get_word_vector(x).tolist())\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=0).fit_transform(np.array(df['fasttext_embeddings'].to_list()))\n",
        "ax = sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], hue=np.array(df['label'].to_list()))\n",
        "sns.move_legend(ax, 'upper left', bbox_to_anchor=(1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce94a816-6d1a-4f69-8f9c-559b24426673",
      "metadata": {
        "id": "ce94a816-6d1a-4f69-8f9c-559b24426673"
      },
      "source": [
        "## Classification\n",
        "\n",
        "Text embeddings can be used as input features in machine learning models, such as classification and clustering. In this example, we use a classification model to predict the disease labels from the embeddings of disease description text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "261e6e78-341b-4b03-902d-3f27922f52d7",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "261e6e78-341b-4b03-902d-3f27922f52d7"
      },
      "outputs": [],
      "source": [
        "# Create a train / test split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(df['embeddings'], df[\"label\"],test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baeff9a8-7be1-4fb9-9cb3-cb44a719076a",
      "metadata": {
        "id": "baeff9a8-7be1-4fb9-9cb3-cb44a719076a"
      },
      "outputs": [],
      "source": [
        "# Normalize features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_x = scaler.fit_transform(train_x.to_list())\n",
        "test_x = scaler.transform(test_x.to_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd8f8a80-6b7e-40a2-82e2-bd8c4be0ab67",
      "metadata": {
        "id": "dd8f8a80-6b7e-40a2-82e2-bd8c4be0ab67"
      },
      "outputs": [],
      "source": [
        "# Train a classifier and compute the test accuracy\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# For a real problem, C should be properly cross validated and the confusion matrix analyzed\n",
        "clf = LogisticRegression(random_state=0, C=1.0, max_iter=500).fit(train_x, train_y.to_list())\n",
        "\n",
        "# you can also try the sag algorithm:\n",
        "# clf = LogisticRegression(random_state=0, C=1.0, max_iter=1000, solver='sag').fit(train_x, train_y)\n",
        "\n",
        "print(f\"Precision: {100*np.mean(clf.predict(test_x) == test_y.to_list()):.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "263c015e-c32d-440c-a7d4-5d977d2d4a6e",
      "metadata": {
        "id": "263c015e-c32d-440c-a7d4-5d977d2d4a6e"
      },
      "outputs": [],
      "source": [
        "# Classify a single example\n",
        "text = \"I've been experiencing frequent headaches and vision problems.\"\n",
        "clf.predict([get_text_embedding([text])])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad4b26bc-2aa0-4008-aad2-7255aaeea3db",
      "metadata": {
        "id": "ad4b26bc-2aa0-4008-aad2-7255aaeea3db"
      },
      "source": [
        "#### Comparison with fasttext\n",
        "Additionally, let’s take a look at the performance using fastText embeddings in this classification task. It appears that the classification model achieves better performance with Mistral embedding models as compared to using fastText embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df2b36ee-0b7d-4813-a259-624a838d2af6",
      "metadata": {
        "id": "df2b36ee-0b7d-4813-a259-624a838d2af6"
      },
      "outputs": [],
      "source": [
        "# Create a train / test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_x, test_x, train_y, test_y = train_test_split(df['fasttext_embeddings'], df[\"label\"],test_size=0.2)\n",
        "# Normalize features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_x = scaler.fit_transform(train_x.to_list())\n",
        "test_x = scaler.transform(test_x.to_list())\n",
        "# Train a classifier and compute the test accuracy\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# For a real problem, C should be properly cross validated and the confusion matrix analyzed\n",
        "clf = LogisticRegression(random_state=0, C=1.0, max_iter=500).fit(train_x, train_y.to_list())\n",
        "# you can also try the sag algorithm:\n",
        "# clf = LogisticRegression(random_state=0, C=1.0, max_iter=1000, solver='sag').fit(train_x, train_y)\n",
        "print(f\"Precision: {100*np.mean(clf.predict(test_x) == test_y.to_list()):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a1a6428-77aa-41f5-bf7a-5af9ea609003",
      "metadata": {
        "id": "5a1a6428-77aa-41f5-bf7a-5af9ea609003"
      },
      "source": [
        "## Clustering\n",
        "\n",
        "What if we don’t have disease labels? One approach to gain insights from the data is through clustering. Clustering is an unsupervised machine learning technique that groups similar data points together based on their similarity with respect to certain features. In the context of text embeddings, we can use the distance between each embedding as a measure of similarity, and group together data points with embeddings that are close to each other in the high-dimensional space.\n",
        "\n",
        "Since we already know there are 24 clusters, let’s use the K-means clustering with 24 clusters. Then we can inspect a few examples and verify whether the examples in a single cluster are similar to one another. For example, take a look at the first three rows of cluster 23. We can see that they look very similar in terms of symptoms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d053db2c-4a89-4f98-a1ed-36594b786ea8",
      "metadata": {
        "id": "d053db2c-4a89-4f98-a1ed-36594b786ea8"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "model = KMeans(n_clusters=24, max_iter=1000)\n",
        "model.fit(df['embeddings'].to_list())\n",
        "df[\"cluster\"] = model.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0169b960-faea-4da6-b509-08558d476118",
      "metadata": {
        "id": "0169b960-faea-4da6-b509-08558d476118"
      },
      "outputs": [],
      "source": [
        "print(*df[df.cluster==23].text.head(3), sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a46a74a-fc5e-463f-9a66-1962b02f96bd",
      "metadata": {
        "id": "3a46a74a-fc5e-463f-9a66-1962b02f96bd"
      },
      "source": [
        "## Retrieval\n",
        "\n",
        "Our embedding model excels in retrieval tasks, as it is trained with retrieval in mind. Embeddings are also incredibly helpful in implementing retrieval-augmented generation (RAG) systems, which use retrieved relevant information from a knowledge base to generate responses. At a high-level, we embed a knowledge base, whether it is a local directory, text files, or internal wikis, into text embeddings and store them in a vector database. Then, based on the user's query, we retrieve the most similar embeddings, which represent the relevant information from the knowledge base. Finally, we feed these relevant embeddings to a large language model to generate a response that is tailored to the user's query and context. If you are interested in learning more about how RAG systems work and how to implement a basic RAG, check out our [previous guide](https://github.com/mistralai/cookbook/blob/main/mistral/rag/basic_RAG.ipynb) on this topic.\n"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "formats": "ipynb,py:light"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}